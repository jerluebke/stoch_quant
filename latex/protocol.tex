\documentclass[11pt,a4paper]{scrartcl}
\usepackage{fontspec}
\usepackage{polyglossia}
    \setdefaultlanguage{english}
\usepackage{lmodern}
\usepackage{fixcmex}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{siunitx}
    \sisetup{%
        range-units=brackets,
        separate-uncertainty=true,
    }
\usepackage{physics}
\usepackage{array}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{graphicx}
    \graphicspath{figures}
\usepackage{tikz}
    \usetikzlibrary{calc,external}
    \tikzexternalize[prefix=extern/]
    \tikzexternaldisable
\usepackage{pgfplots}
    \pgfplotsset{%
        compat=1.15,
        table/search path={data},
    }
\usepackage[makeroom]{cancel}
\usepackage{todonotes}
\usepackage[parfill]{parskip}
\usepackage[%
    colorlinks=true, linkcolor=blue,
    % hidelinks
]{hyperref}
\usepackage{cleveref}


\newcommand*{\figref}[1]{(see fig.~\ref{#1})}
\newcommand{\eg}{e.\,g.}
\newcommand{\ie}{i.\,e.}
\newcommand{\action}{\ensuremath{\mathcal{S}}}
\newcommand{\SD}{\ensuremath{\mathcal{S}_{\mathrm{D}}}}
\newcommand{\xdoti}{\ensuremath{\dot{x}_i}}
\newcommand{\xdotj}{\ensuremath{\dot{x}_j}}
\newcommand{\xdotjm}{\ensuremath{\dot{x}_{j-1}}}
\newcommand{\deltaij}{\ensuremath{\delta_{ij}}}
\newcommand{\OverDeltaT}[1]{\ensuremath{\frac{#1}{\Delta t}}}
\newcommand{\DT}{\ensuremath{\mathrm{D}_t}}
\newcommand{\OverTwoDeltaT}[1]{\ensuremath{\frac{#1}{2\, \Delta t}}}
\newcommand{\xdotjp}{\ensuremath{\dot{x}_{j+1}}}
\newcommand{\avg}[1]{\langle#1\rangle}
\newcommand{\SE}{\ensuremath{\mathcal{S}_{\mathrm{E}}}}
\newcommand{\SEh}{\mathcal{S}_{\mathrm{E},h}}
\newcommand{\pathinth}[1]{\int\mathcal{D}[x]#1e^{-\SEh}}
\newcommand{\pathint}[1]{\int\mathcal{D}[x]#1e^{-\SE}}
\newcommand{\BigO}[1]{\mathcal{O}\left(#1\right)}
\newcommand{\tE}{\ensuremath{t_{\mathrm{E}}}}
\DeclareMathOperator{\cov}{cov}


\title{Extensions to the \emph{Stochastic Quantization Manual}}
\author{Jeremiah LÃ¼bke, Franz Wilfarth}
\date{\today}


\begin{document}

\maketitle

\subsubsection*{1.2.1 Parabolic Potential}
\dots

The harmonic oscillator is described by the Euclidean action
\begin{align}
    \action_\mathrm{E}\left[x(t)\right] =
    \int\dd{t_\mathrm{E}}\left[\frac{m}{2}\dot{x}^2 +
        \frac{1}{2}\,m\,\omega^2\,x^2\right]
\end{align}
with $x$ being $x = x(t, \tau )$ and the dot denoting the derivative with
respect to $t_\mathrm{E}$ (for more details, \todo{link appears when using
\texttt{section} instead of \texttt{section*}}see~\ref{app:derivSE}).

\dots


\section*{2 Numerical Approach and Discretization for 1-dimensional Quantum
Mechanics}
\dots Therefore consider a discretization of \todo{add reference}eq.~(1.6) in
both the Euclidean time $t$ and the fictitious time $\tau$ (bearing in mind the
notes \todo{see above}in~\ref{app:derivSE} and~\ref{app:deriv-discrete}):

\dots

The equivalence of those expressions can be easily verified by calculating the
derivative
\begin{align*}
    \eval{\dv{\avg{x_l}_h}{h}}_{h=0} &=
    \eval{\dv{h}\frac{\pathinth{\,x_l\,}}{\pathinth{}}}_{h=0} \\ &=
    \eval{\frac{\left(\pathinth{\,x_l x_0\,}\right)\left(\pathinth{}\right)
    -\left(\pathinth{x_l}\right)\left(\pathinth{x_0}\right)}
    {\left(\pathinth{}\right)^2}}_{h=0} \\ &=
    \frac{\left(\pathint{\,x_0 x_l\,}\right)\left(\pathint{}\right)
    -\left(\pathint{x_l}\right)\left(\pathint{x_0}\right)}
    {\left(\pathint{}\right)^2} \\ &=
    \avg{x_0 x_l}-\avg{x_0}\avg{x_l} \tag{i}\label{eq:i}
\end{align*}
Note the Taylor expansion at $h=0$ (where $\avg{x_l}_0\equiv\avg{x_l}$):
\begin{equation*}
    \avg{x_l}_h=\avg{x_l}_0+\eval{\dv{\avg{x_l}_h}{h}}_{h=0}\,h+\BigO{h^2}
    \tag{ii}\label{eq:ii}
\end{equation*}
Combining equation (\ref{eq:i}) and (\ref{eq:ii}) yields above expression.

\dots


\subsection*{4.3 Problems with the double-well potential}
When attempting to find the first energy gap of the double-well potential
\begin{equation}
    V(x)=h\left(x^2-a^2\right)^2
    \label{eq:double-well}
\end{equation}
where $h$ is the height of the central barrier and $2\,a$ is the distance of the
minima, one finds that the SQM-algorithm converges \emph{very} slowly (we were not
able to obtain meaningful results during our investigation).

Why is that so? At first recall, that the energy gap is computed via
approximating the correlation function\todo{add reference to eq. 1.12 in the manual}
\begin{align*}
    \ev{x_0\,x_t}{0}&=\sum_{n}e^{-(E_n-E_0)\,t}\mel{0}{x_0}{n}\mel{n}{x_0}{0}
    \\&\approx{e^{-\Delta{E_1}\,t}}\abs{\mel{0}{x_0}{1}}^2
\end{align*}
which is only valid for sufficiently large succeeding energy levels, causing
the remaining terms to decay exponentially. However in our case the first two
energy levels lie rather close to each other\footnote{numerically solving the
eigenvalues of the associated Hamiltonian for $m=1$, $h=1$, $a=1$ yields
$\Delta{E_2}\approx0.79$}, invalidating the approximation.

On the other hand -- from a numerical point of view -- the term
\begin{equation*}
    x_l(\tau_{i+1})=x_l(\tau_i)-\pdv{V(x_l)}{x_l}\Delta\tau+\dotsb
\end{equation*}
corresponds to a gradient descent, which means the algorithm samples the
minimum of a given potential to find the lowest energy gap. Now the double-well
potential has two minima, which both need to be sampled sufficiently in order
to account for the splitting of the energy levels. To achieve this, the
simulated trajectory needs to tunnel back and forth through the barrier.

In this context it is interesting to consider the number of transitions and
observe its change for varying heights. This was done by performing a small
fixed number of simulation steps (\textrightarrow~\emph{multistep}) and
computing a short-term average over this span, which exhibits the small
time-scales of the system, in contrast to the regular long-term average. When
plotting the first, one finds that the trajectory behaves much more dynamically
-- \eg~it tunnels between the two minima --, while the latter approaches some
stable limit which lies between the minima. In order to quantify the number of
transitions, a point in the middle of the trajectory was observed and a
counter was incremented whenever its sign changed (\ie~it tunneled)
\figref{fig:avg}.

\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{figures/averages}
    \caption{averaged trajectory after $\approx\num{64000}$ steps and
    short-term average over \num{100} steps}
    \label{fig:avg}
\end{figure}

Our measurements were recorded with fixed $a=1$ and $h\in[0.5,16]$; for each
$h$, \num{10000} mutlisteps were performed, each consisting of 100 simulation
steps. This was repeated 16 times with different seeds for \texttt{numpy}'s
PRNG and eventually mean and standard deviation were computed. The obtained
data closely resembles an exponential relation
\begin{equation*}
    N(h)=N_0\,\exp\left(-\frac{h}{\tau}\right)
\end{equation*}
and indeed, when performing a linear regression on
$\log(N_{\mathrm{measured}})$, one finds that it closely follows the linear
relation
\begin{equation*}
    \log\left(N(h)\right)=m\,h+b
\end{equation*}
with $m=-\tau^{-1}$ and $b=\log{N_0}$ \figref{fig:transitions}. Via error
propagation, one finds
$\tau=\num{3.58(4)}$.

\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{figures/transitions}
    \caption{Decay of the tunneling events with increasing height of the potential's barrier}
    \label{fig:transitions}
\end{figure}

This clearly shows, that the number of transitions decreases exponentially with
increasing $h$, and therewith requiring much more simulation steps to obtain
meaningful results.


\section*{5 Conclusion and Outlook}
\dots

Having understood the basics and technicalities of SQM for these
examples, the next interesting step would be the double-well potential
$V(x)=h\left(x^2-a^2\right)^2$.
However, when trying to apply SQM directly to compute the first energy gap of
this potential, one quickly finds this task to be non-trivial, since it
requires a huge amount of simulation steps in order account for the splitting
of the lowest energy levels by sufficiently sampling both minima. This requires
a sufficient number of transitions, which exponentially decreases for
increasing heights $h$, in turn requiring even more simulation steps for
meaningful results. In this situation the advantage of the Parisi trick
vanishes and one needs to think of a more clever way to address this problem.

An alternative way could be provided by doing stochastic quantization
\enquote{around} the fluctuating instantons in the double well (see \todo{add
reference}[MY86]). In general \dots




\section*{C On the functional derivative of the action \action}
\subsection*{C.1 The euclidean action \SE}
\label{app:derivSE}
Assume the Lagrangian to be $L(x,\dot{x})=\frac{m}{2} \dot{x}^2-V(x)$. So the
action $\action$ can be written as:
\begin{align}
    \action\left[x(t)\right] = \int\limits_{t_0}^{t_1}\dd{t}L\left(x(t), \dot{x}(t)\right) = \int\limits_{t_0}^{t_1}\dd{t}\left[\frac{m}{2}\dot{x}^2-V(x)\right]\label{eq:S}
\end{align}
Note that by Wick rotation $t\longrightarrow \tE:=it$. Now $L(x,\dot{x})$ is
dependent on \tE.
\begin{align}
    \action\longrightarrow\SE:=-i\action
    &=\int\dd{\tE}L_\mathrm{E}\left(x(t_\mathrm{E}),
    \dot{x}(t_\mathrm{E})\right) \nonumber \\
    &=-i\int\dd\frac{\tE}{i}\left[\frac{m}{2}\left(\pdv{x}{(\tE/i)}\right)^2-V(x)\right]
    \nonumber \\
    &=-\int\dd\tE\left[-\frac{m}{2}\left(\pdv{x}{\tE}\right)^2-V(x)\right]
    \nonumber \\
    &=\int\dd\tE\left[\frac{m}{2}\dot{x}^2+V(x)\right] \label{eq:SE}
\end{align}

By comparison of \cref{eq:S} and \cref{eq:SE} it can be seen that the
Lagrangian of the Wick rotated action has a different sign in front of it's
potential, resulting in the following Euler-Lagrange-Equation:
\begin{equation}
    \boxed{%
        \fdv{\SE}{x}=\pdv{L}{x}-\dv{\tE}\pdv{L}{\dot{x}}=-m\ddot{x}+\pdv{V}{x}
    }
\end{equation}


\subsection*{C.2 Discrete functional derivative}
\label{app:deriv-discrete}
In order to show the correspondence between the continuous functional
derivative and its application in a discrete framework, the derivative of the
discretized action is computed.

Let us start with discretizing the functional:
\begin{equation}
    \mathcal{S}\left[x(t)\right] = \int\dd{t}L\left(x(t), \dot{x}(t)\right)
    \quad\longrightarrow\quad
    \SD\left(\{x_i\}\right) = \sum_i L\left(x_i, \xdoti\right)\Delta t
\end{equation}
where $x(t) \longrightarrow x_i = x(t_i)$ and $\dot{x}(t) \longrightarrow
\xdoti = \OverTwoDeltaT{x_{i+1}-x_{i-1}}$. For convenience we write $L_i = L(x_i, \xdoti)$.

We compute the derivative:
\begin{equation}
    \dv{\SD}{x_j} =\sum_i\dv{L_i}{x_j}\,\Delta t
    =\sum_i\left(\pdv{L_i}{x_i}\pdv{x_i}{x_j}+\pdv{L_i}{\xdoti}\pdv{\xdoti}{x_j}\right)
    \Delta t
\end{equation}
with
\begin{subequations}
\begin{gather}
    \pdv{x_i}{x_j}=\OverDeltaT{\deltaij} \\
    \pdv{\xdoti}{x_j}=\pdv{\OverTwoDeltaT{x_{i+1}-x_{i-1}}}{x_j}
    =\OverTwoDeltaT{1}\left(\pdv{x_{i+1}}{x_j}-\pdv{x_{i-1}}{x_j}\right)
    =\frac{\delta_{i+1,j}-\delta_{i-1,j}}{2\,(\Delta t)^2}.
\end{gather}
\end{subequations}

The Kronecker $\delta$-s kill the sum, and we can write (pay attention to the indices):
\begin{align*}
    \dv{\SD}{x_j}&=\left[\OverDeltaT{1}\pdv{L_j}{x_j}+\frac{1}{2\,(\Delta{t})^2}
    \left(\pdv{L_{j-1}}{\xdotjm}-\pdv{L_{j+1}}{\xdotjp}\right)\right]\Delta{t}
    \\
    &=\pdv{L_j}{x_j}-\OverTwoDeltaT{1}\left(\pdv{L_{j+1}}{\xdotjp}-\pdv{L_{j-1}}{\xdotjm}\right)
\end{align*}
\begin{equation}
    \implies\boxed{\dv{\SD}{x_j}=\pdv{L_j}{x_j}-\DT\left(\pdv{L_{j}}{\xdotj}\right)}
\end{equation}
where $\DT(f_i)=\OverTwoDeltaT{f_{i+1}-f_{i-1}}$ is the discrete time derivation
operator.
This is exactly what we wanted to achieve.

For clarity, we make the transition back to the continuous description:
\begin{equation}
    \dv{\SD}{x_j}\longrightarrow\fdv{\mathcal{S}[x]}{x}=\pdv{L}{x}-\dv{t}\pdv{L}{\dot{x}}
\end{equation}
which is the well known \emph{Euler-Lagrange-Equation}.


\end{document}

% vim: set ff=unix tw=79 sw=4 ts=4 et ic ai :
