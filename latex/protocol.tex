\section{Results}
\subsection{Problems with the double-well potential}
When attempting to find the first energy gap of the double-well potential
\begin{equation}
    V(x)=h\left(x^2-a^2\right)^2
    \label{eq:double-well}
\end{equation}
where $h$ is the height of the central barrier and $2\,a$ is the distance of the
minima, one finds that the SQM-algorithm converges \emph{very} slowly (we were not
able to obtain meaningful results during our investigation).

Why is that so? At first recall, that the energy gap is computed via
approximating the correlation function\todo{add reference to eq. 1.12 in the manual}
\begin{align*}
    \ev{x_0\,x_t}{0}&=\sum_{n}e^{-(E_n-E_0)\,t}\mel{0}{x_0}{n}\mel{n}{x_0}{0}
    \\&\approx{e^{-\Delta{E_1}\,t}}\abs{\mel{0}{x_0}{1}}^2
\end{align*}
which is only valid for sufficiently large succeeding energy levels, causing
the remaining terms to decay exponentially. However in our case the first two
energy levels lie rather close to each other\todo{verify}\footnote{numerically solving the
eigenvalues of the associated Hamiltonian for $m=1$, $h=1$, $a=1$ yields
$\Delta{E_2}\approx0.79$}, invalidating the approximation.

On the other hand from a numerical point of view, the term
\begin{equation*}
    x_l(\tau_{i+1})=x_l(\tau_i)-\pdv{V(x_l)}{x_l}\Delta\tau+\dotsb
\end{equation*}
corresponds to a gradient descent, which means the algorithm samples the
minimum of a given potential to find the lowest energy gap. Now the double-well
potential has two minima, which both need to be sampled sufficiently in order
to account for the splitting of the energy levels. To achieve this, the
simulated trajectory needs to tunnel back and forth through the barrier.

In this context it is interesting to consider the number of transitions and
observe its change for varying heights. This was done by performing a small
fixed number of simulation steps (\textrightarrow~\emph{multistep}) and
computing a short-term average over this span, which exhibits the small
time-scales of the system, in contrast to the regular long-term average. When
plotting the first, one finds that the trajectory behaves much more dynamically
-- \eg~it tunnels between the two minima --, while the latter approaches some
stable limit which lies between the minima. In order to quantify the number of
transitions, a point in the middle of the trajectory was observed and a
counter was incremented whenever its sign changed (\ie~it tunneled)
\figref{fig:avg}.

\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{figures/averages}
    \caption{averaged trajectory after $\approx\num{64000}$ steps and
    short-term average over \num{100} steps}
    \label{fig:avg}
\end{figure}

Our measurements were recorded with fixed $a=1$ and $h\in[0.5,16]$; for each
$h$, \num{10000} mutlisteps were performed, each consisting of 100 simulation
steps. This was repeated 16 times with different seeds for \texttt{numpy}'s
PRNG and eventually mean and standard deviation were computed. The obtained
data closely resembles an exponential relation
\begin{equation*}
    N(h)=N_0\,\exp\left(-\frac{h}{\tau}\right)
\end{equation*}
and indeed, when performing a linear regression on
$\log(N_{\mathrm{measured}})$, one finds that it closely follows the linear
relation
\begin{equation*}
    \log\left(N(h)\right)=m\,h+b
\end{equation*}
with $m=-\tau^{-1}$ and $b=\log{N_0}$ \figref{fig:transitions}. Via error
propagation, one finds
$\tau=\num{3.58(4)}$.

\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{figures/transitions}
    \caption{Decay of the tunneling events with increasing height of the potential's barrier}
    \label{fig:transitions}
\end{figure}

This clearly shows, that the number of transitions decreases exponentially with
increasing $h$, and therewith requiring much more simulation steps to obtain
meaningful results.


\newpage

\section{Conclusion and Outlook}
\dots Having understood the basics and technicalities of SQM for these
examples, the next interesting step would be the double-well potential
$V(x)=h\left(x^2-a^2\right)^2$.
However, when trying to apply SQM directly to compute the first energy gap of
this potential, one quickly finds this task to be non-trivial, since it
requires a huge amount of simulation steps in order account for the splitting
of the lowest energy levels by sufficiently sampling both minima. This requires
a sufficient number of transitions, which exponentially decreases for
increasing heights $h$, in turn requiring even more simulation steps for
meaningful results. In this situation the advantage of the Parisi trick
vanishes and one needs to think of a more clever way to address this problem.

An alternative way could be provided by doing stochastic quantization
\enquote{around} the fluctuating instantons in the double well (see \todo{add
reference}[MY86]). In general \dots


\vspace{5\baselineskip}


\todo[inline]{discrete action derivation - I don't like the derivation, cause
you don't know how a second derivative could be calculated.}

\newpage

\subsection{From Stochastic to Quantum Systems}
...\\
\noindent The variable $x(t,\tau)$ gets discretized in time t. Note that x is is still continuously dependent on $\tau$. The Langevin-Equation is valid for every time step $t_i$. The two equations giving a proper description to the desired fictitious stochastic system
are the Langevin and the corresponding Fokker-Planck equation (A.18) 2\todo{here original links} , which shall be
given in the form needed for the simulation

\begin{align}
    \dv{x_i}{\tau} &= A_i + \zeta_i\\
    \pdv{p}{\tau} &= - \frac{1}{\Delta t} \pdv{A_i p}{x_i} + \frac{1}{\Delta t^2} \pdv[2]{p}{x_i}\\
    \langle \zeta_i \left(\tau \right) \zeta_j \left( \tau' \right) \rangle &= 2 \frac{\delta_{ij}}{\Delta t} \delta\left( \tau - \tau' \right)
\end{align}
\noindent A stationary solution to this problem implies
\begin{align}
    \pdv{A_i p}{x_i} = \frac{1}{\Delta t} \pdv{p}{x_i} + const.
\end{align}
where the constant is set to be zero to ensure the proper decay of p in the infinity \todo{"infinity" war im Protokoll an dieser Stelle falsch  geschrieben} limit. Now let A be derived from a potential $V(x)$ via $A_i = - \frac{1}{\Delta t} \pdv{V}{x_i}$ which immediately results in a stationary solution of the form

\begin{align}
    p = c \cdot e^{-V(x)}
\end{align}{}

\noindent So the Langevin and the Fokker-Planck equation can be rewritten as
\begin{align}
    \dv{x_i}{\tau} &= - \frac{1}{\Delta t} \pdv{V}{x_i} + \zeta_i\\
    \pdv{p}{\tau} &= - \frac{1}{\Delta t} \pdv{x_i} \left( - \frac{1}{\Delta t} \pdv{V}{x_i} p \right) + \frac{1}{\Delta t^2} \pdv[2]{p}{x_i}
\end{align}{}

...\\
When performing the limit one has to take into account that the partial derivatives $\dv{x_i}$ / $\partial_i$ \todo{Which Schreibweise should be used? The short one of the original protocol or our one?} become a functional derivative $\fdv{x}$. To switch between discretized and continuous formulation after [Kis00] it is given that:
\begin{equation}
    \fdv{x} = \lim\limits_{\Delta t \to 0} \OverDeltaT{1} \pdv{x_i}
\end{equation}
...\\
\\
\subsection{Parabolic Potential}

...\\
The harmonic oscillator is described by the Euclidean action
\begin{align}
    \action_\mathrm{E}\left[x(t)\right] = \int\dd{t_\mathrm{E}}\left[\frac{m}{2}\dot{x}^2 + \frac{1}{2} m \omega x^2\right]
\end{align}
with $x$ being $x = x(t, \tau )$ and the dot denoting the derivative with respect to $t_\mathrm{E}$ (\ref{app:derivSE}).\\
...

\subsection{Numerical Approach and Discretization
for 1-dimensional Quantum Mechanics}
...\\
The equivalence of those expressions can be easily verified by calculating the derivative\\
\begin{align*}
    \eval{\dv{\avg{x_l}_h}{h}}_{h=0} &=
    \eval{\dv{h}\frac{\pathinth{\,x_l\,}}{\pathinth{}}}_{h=0} \\ &=
    \eval{\frac{\left(\pathinth{\,x_l x_0\,}\right)\left(\pathinth{}\right)
    -\left(\pathinth{x_l}\right)\left(\pathinth{x_0}\right)}
    {\left(\pathinth{}\right)^2}}_{h=0} \\ &=
    \frac{\left(\pathint{\,x_0 x_l\,}\right)\left(\pathint{}\right)
    -\left(\pathint{x_l}\right)\left(\pathint{x_0}\right)}
    {\left(\pathint{}\right)^2} \\ &=
    \avg{x_0 x_l}-\avg{x_0}\avg{x_l} \tag{i}\label{eq:i}
\end{align*}
Note the Taylor expansion at $h=0$ (where $\avg{x_l}_0:=\avg{x_l}$):
\begin{equation*}
    \avg{x_l}_h=\avg{x_l}_0+\eval{\dv{\avg{x_l}_h}{h}}_{h=0}\,h+\BigO{h^2}
    \tag{ii}\label{eq:ii}
\end{equation*}
Combining equation (\ref{eq:i}) and (\ref{eq:ii}) the expression above is obtained.\\
...\\

\newpage
\subsection{Appendix: Functional derivative of $\SE$\label{app:derivSE}}

Assume the Lagrangian to be $L(x,\dot{x})=\frac{m}{2} \dot{x}^2-V(x)$. So the action $\action$ can be written as:
\begin{align}
    \action\left[x(t)\right] = \int\limits_{t_0}^{t_1}\dd{t}L\left(x(t), \dot{x}(t)\right) = \int\limits_{t_0}^{t_1}\dd{t}\left[\frac{m}{2}\dot{x}^2-V(x)\right]\label{eq:S}
\end{align}
\noindent Note that by Wick rotation $t\longrightarrow \tE:=it$. Now $L(x,\dot{x})$ is dependent on $\tE$.
\begin{align}
    \action\longrightarrow\SE:&=-i\action
    =-i\int\dd\frac{\tE}{i}\left[\frac{m}{2}\left(\pdv{x}{(\tE/i)}\right)^2-V(x)\right] \\
    &=-\int\dd\tE\left[-\frac{m}{2}\left(\pdv{x}{\tE}\right)^2-V(x)\right] \\
    &=\int\dd\tE\left[\frac{m}{2}\dot{x}^2+V(x)\right] =  \int\dd\tE L_\mathrm{E}\left(x(t_\mathrm{E}), \dot{x}(t_\mathrm{E})\right)\label{eq:SE}
\end{align}

\noindent By comparison of \cref{eq:S} and \cref{eq:SE} it can be seen that the Lagrangian of the Wick rotated action has another sign before it's potential: $L_\mathrm{E}(x,\dot{x})=\frac{m}{2} \dot{x}^2+V(x)$.

\noindent The functional derivative is defined in (\ref{eq:func}) where $\Phi$ is an arbitrary function vanishing at the endpoints of the path.
\begin{align}{}
    \int\limits_{t_0}^{t_1} \dd{t} \fdv{\action}{x} \Phi (t) &= \dv{\varepsilon} \action [y(t)]\bigg|_{\varepsilon = 0}\quad \text{where} \quad y(t) = x(t) + \varepsilon \Phi(t) \label{eq:func}\\
    &= \dv{\varepsilon} \int\limits_{t_0}^{t_1}\dd{t}L\left(y, \dot{y}(t)\right)\bigg|_{\varepsilon = 0} = \int\limits_{t_0}^{t_1}\dd{t}\pdv{L}{y}\dv{y}{\varepsilon} + \pdv{L}{\dot{y}}\dv{\dot{y}}{\varepsilon}\bigg|_{\varepsilon = 0}\\
    &= \int\limits_{t_0}^{t_1}\dd{t}\pdv{L}{x}\Phi + \pdv{L}{\dot{x}}\dot{\Phi} = \int\limits_{t_0}^{t_1}\dd{t}\pdv{L}{x}\Phi + \left[ \pdv{L}{\dot{x}} \Phi\right]_{t_0}^{t_1}- \int \limits_{t_0}^{t_1} \dd{t} \dv{t}\pdv{L}{\dot{x}}\Phi\\
    &= \int\limits_{t_0}^{t_1}\dd{t}\left( \pdv{L}{x} - \dv{t}\pdv{L}{\dot{x}} \right)\Phi
\end{align}
Finally the functional derivatives of $\action$ and $\action_\mathrm{E}$ can be written as:
\begin{align}{}
    \fdv{\action}{x}&= \pdv{L}{x} - \dv{t} \pdv{L}{\dot{x}} = - \pdv{V}{x} - m \ddot{x}\\
    \fdv{\action_\mathrm{E}}{x}&= \pdv{L_\mathrm{E}}{x} - \dv{t_\mathrm{E}} \pdv{L_\mathrm{E}}{\dot{x}} = \pdv{V}{x} - m \ddot{x}
\end{align}{}
\\
\todo{protokoll Seite 24, A.19: $\partial_j$ falsch}

\begin{align}
    \dv{\varepsilon} \action [y(t)]\bigg|_{\varepsilon = 0} &= \lim_{\varepsilon \to 0} \frac{1}{\varepsilon}(\action[y(t)] - \action[x(t)])\quad \text{where} \quad y(t) = x(t) + \varepsilon \Phi(t)\\
    \fdv{\action}{x}&= \lim_{\varepsilon \to 0} \frac{1}{\varepsilon}(\action[x(t) + \varepsilon \delta (t-t')] - \action[x(t)])\label{eq:deltafunc}
\end{align}{}

An easy way to calculate $\fdv{\action}{x}$ is given by \cref{eq:deltafunc}.\todo{Weiß nicht, ob das folgende dir in deinen Überlegungen hilft, aber das wurde auch noch in [Kis00] angeführt...}

% \subsection{Appendix: Discrete derivative of $\action$}

% Let us start with discretizing the functional:
% \begin{equation*}
%     \mathcal{S}\left[x(t)\right] = \int\dd{t}L\left(x(t), \dot{x}(t)\right)
%     \quad\longrightarrow\quad
%     \SD\left(\{x_i\}\right) = \sum_i L\left(x_i, \xdoti\right)\Delta t
% \end{equation*}
% where $x(t) \longrightarrow x_i = x(t_i)$ and $\dot{x}(t) \longrightarrow
% \xdoti = \OverTwoDeltaT{x_{i+1}-x_{i-1}}$. For convenience we write $L_i = L(x_i, \xdoti)$.\\

% \noindent We compute the derivative:
% \begin{equation*}
%     \dv{\SD}{x_j} =\sum_i\dv{L_i}{x_j}\,\Delta t
%     =\sum_i\left(\pdv{L_i}{x_i}\pdv{x_i}{x_j}+\pdv{L_i}{\xdoti}\pdv{\xdoti}{x_j}\right)
%     \Delta t
% \end{equation*}
% with
% \begin{equation*}
%     \pdv{x_i}{x_j}=\OverDeltaT{\deltaij}
% \end{equation*}
% and
% \begin{align*}
%     \pdv{\xdoti}{x_j}=\pdv{\OverTwoDeltaT{x_{i+1}-x_{i-1}}}{x_j}
%     =\OverTwoDeltaT{1}\left(\pdv{x_{i+1}}{x_j}-\pdv{x_{i-1}}{x_j}\right)
%     =\frac{\delta_{i+1,j}-\delta_{i-1,j}}{2\,(\Delta t)^2}.
% \end{align*}

% \noindent The Kronecker $\delta$-s kill the sum, and we can write (pay attention to the indices):
% \begin{align*}
%     \dv{\SD}{x_j}&=\left[\OverDeltaT{1}\pdv{L_j}{x_j}+\frac{1}{2\,(\Delta{t})^2}
%     \left(\pdv{L_{j-1}}{\xdotjm}-\pdv{L_{j+1}}{\xdotjp}\right)\right]\Delta{t}\\
%     &=\pdv{L_j}{x_j}-\OverTwoDeltaT{1}\left(\pdv{L_{j+1}}{\xdotjp}-\pdv{L_{j-1}}{\xdotjm}\right)\\
%     \implies\Aboxed{\dv{\SD}{x_j}&=\pdv{L_j}{x_j}-\DT\left(\pdv{L_{j}}{\xdotj}\right)}
% \end{align*}
% where $\DT(f_i)=\OverTwoDeltaT{f_{i+1}-f_{i-1}}$ is the discrete time derivation
% operator.\\
% This is exactly what we wanted to achieve.\\

% For clarity, we make the transition back to the continuous description:
% \begin{equation*}
%     \dv{\SD}{x_j}\longrightarrow\fdv{\mathcal{S}[x]}{x}=\pdv{L}{x}-\dv{t}\pdv{L}{\dot{x}}
% \end{equation*}
% which is the well known \emph{Euler-Lagrange-Equation}.

vim: set ff=unix tw=79 sw=4 ts=4 et ic ai :
